<skill id="lollms-instantiation">
    <name>Client Setup &amp; Bindings</name>
    <description>Initializing LollmsClient with different backends (Ollama, OpenAI, LoLLMs, etc.).</description>
    <category>python/lollms_client/setup</category>
    <language>markdown</language>
    <timestamp>1738425600000</timestamp>
    <content>
        <![CDATA[
# LoLLMs Client Setup

## 1. Basic Setup (LoLLMs Server)
```python
from lollms_client import LollmsClient

lc = LollmsClient(
    llm_binding_name="lollms",
    llm_binding_config={
        "host_address": "http://localhost:9642", # Mandatory
        "model_name":"mistral", # Mandatory
        "service_key":"lollms_SOMERANDOMSEQUENCE", # Mandatory by default but can be deactivated
        "verify_ssl_certificate":True, # Optional (default: True) Can be deactivated if the server has no verifiable authority
        "certificate_file_name":"path/to/certificate/file" # Optional, if the server has no verifiable authority but we do have a certificate file to verify, we can use this 
    }
)
```

## 2. Ollama Binding
```python
lc = LollmsClient(
    llm_binding_name="ollama",
    llm_binding_config={
        "host_address": "http://localhost:11434" # Mandatory
        "model_name": "llama3", # Mandatory
        # we can also use these if we are using ollama proxy server
        "service_key":"lollms_SOMERANDOMSEQUENCE",
        "verify_ssl_certificate":True,
        "certificate_file_name":"path/to/certificate/file"
)
```

## 3. OpenAI Binding
```python
import os
lc = LollmsClient(
    llm_binding_name="openai",
    llm_binding_config={
        "host_address": "http://localhost:11434" # Optional (defaults to open ai server), it can be changed to use an open ai compatible server
        "model_name": "gpt-4o",
        "service_key": "sk_THEOPENAIKEYORANOTHERCOMPATIBLESERVICEKEY"
        # we can also use these if we are using ollama proxy server
        "verify_ssl_certificate":True,
        "certificate_file_name":"path/to/certificate/file"
    }
)
```

## 4. Local GGUF (LlamaCpp)
```python
lc = LollmsClient(
    llm_binding_name="llama_cpp_server",
    llm_binding_config={
        "models_path": "/path/to/models/flder", # Mandatory
        "model_name": "mistral.gguf", # Mandatory
        # Optional configurations
        "n_gpu_layers": -1,
        "n_ctx": 128000
    }
)
```

## Other compatible bindings
### OpenAi style configuration:

claude (anthropic), gemini(google), groq, grok(Xai), litellm, open_router


]]>
    </content>
</skill>